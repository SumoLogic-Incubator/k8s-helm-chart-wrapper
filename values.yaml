## Fill in all the configuration options that you normally would for
## https://github.com/SumoLogic/sumologic-kubernetes-collection/tree/main/deploy/helm/sumologic
sumologic:
  nameOverride: ""
  fullnameOverride: ""

  sumologic:
    # accessId: ""
    # accessKey: ""
    # clusterName: ""
  
  fluentd:
    image:
      tag: 1.12.2-sumo-6
    metrics:
      # The below `record_transformer` snippet adds a `k8s.cronjob.name` label on all metrics that describe cronjob pods.
      extraFilterPluginConf: |-
        <filter prometheus.metrics**>
          @type record_transformer
          enable_ruby
          <record>
            k8s.cronjob.name ${record.dig("pod_labels", "job-name")&.sub(/-\d+$/, '')}
          </record>
        </filter>

  ## Keep this disabled to prevent deploying kube-prometheus-stack subchart.
  kube-prometheus-stack:
    enabled: false

  fluent-bit:
    ## fluent-bit's subchart name overrides
    nameOverride: ""
    fullnameOverride: ""

## This chart's names overrides
nameOverride: ""
fullnameOverride: ""

## Customize the Prometheus deployment
prometheusSpec:
  scrapeInterval: "30s"
  retention: 6h
  resources:
    limits:
      cpu: "2"
      memory: 8Gi
    requests:
      cpu: "2"
      memory: 8Gi

  ## Define which Nodes the Pods are scheduled on.
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Namespaces to be selected for PrometheusRules discovery.
  ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
  ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  ##
  ruleNamespaceSelector: {}

  ## PrometheusRules to be selected for target discovery.
  ## If {}, select all ServiceMonitors
  ##
  ## Example which select all prometheusrules resources
  ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
  # ruleSelector:
  #   matchExpressions:
  #     - key: prometheus
  #       operator: In
  #       values:
  #         - example-rules
  #         - example-rules-2
  #
  ## Example which select all prometheusrules resources with label "role" set to "example-rules"
  # ruleSelector:
  #   matchLabels:
  #     role: example-rules
  ruleSelector:
    matchLabels:
      k8c.collection.enabled: "true"

  ## Namespaces to be selected for ServiceMonitor discovery.
  ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  ##
  serviceMonitorNamespaceSelector: {}

  ## ServiceMonitors to be selected for target discovery.
  ## If {}, select all ServiceMonitors
  ##
  ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
  # serviceMonitorSelector:
  #   matchLabels:
  #     prometheus: somelabel
  serviceMonitorSelector:
    matchLabels:
      k8c.collection.enabled: "true"

  ## Specify additional remote write rules.
  ## For instance:
  ##
  # additionalRemoteWrite:
  # - url: http://$(COLLECTION_SERVICE).$(COLLECTION_NAMESPACE).svc.cluster.local:9888/prometheus.metrics
  #   remoteTimeout: 5s
  #   writeRelabelConfigs:
  #     - action: keep
  #       regex: (?:my_custom_metric_regex.*)
  #       sourceLabels: [__name__]
  additionalRemoteWrite: {}

  kubelet:
    ## Regex which metrics names have to match in order to be scraped and forwarded to Sumo
    ## For default regex look into templates/_helpers.tpl
    metricsRegex: ""

  cadvisor:
    ## Regex which metrics names have to match in order to be scraped and forwarded to Sumo
    ## For default regex look into templates/_helpers.tpl
    ##
    ## Because the container metrics and aggregate metrics are being treated a bit
    ## differently we separate those 2 regexes. One of the reasons is that we drop
    ## the time series for container metrics which do not have a container tag
    ## attached to it.
    containerMetricsRegex: ""
    ## Regex which metrics names have to match in order to be scraped and forwarded to Sumo
    ## For default regex look into templates/_helpers.tpl
    aggregateMetricsRegex: ""

  kubeStateMetrics:
    ## Namespace in which kube-state-metrics is deployed. Required.
    namespace: ""
    ## ServiceMonitor selector for kube-state-metrics deployment's pods.
    ## Typically using 2 labels for maching would be enough:
    ## * app.kubernetes.io/instance
    ## * app.kubernetes.io/name
    ##
    ## If one does not specify that, only matching by
    ## app.kubernetes.io/name=kube-state-metrics is being performed.
    selector: {}
    ## Regex which metrics names have to match in order to be scraped and forwarded to Sumo
    ## For default regex look into templates/_helpers.tpl
    metricsRegex: ""

  nodeExporter:
    ## Regex which metrics names have to match in order to be scraped and forwarded to Sumo
    ## For default regex look into templates/_helpers.tpl
    metricsRegex: ""
    ## Namespace in which node-exporter is deployed. Required.
    namespace: ""
    ## ServiceMonitor selector for node-exporter's pods.
    ## Typically using 2 labels for maching would be enough:
    ## * app
    ## * release
    ##
    ## If one does not specify that, only matching by
    ## app=prometheus-node-exporter is being performed.
    selector: {}

  ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
  ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
  ## as specified in the official Prometheus documentation:
  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
  ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
  ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
  ## scrape configs are going to break Prometheus after the upgrade.
  ##
  ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
  ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
  ##
  additionalScrapeConfigs: []
